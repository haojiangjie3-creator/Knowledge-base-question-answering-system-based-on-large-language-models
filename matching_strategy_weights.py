from sentence_transformers import SentenceTransformer
from project_database import ProjectDatabase
from document_extraction_strategy import extract_features_from_document
import numpy as np
from text2vec import SentenceModel
from sklearn.metrics.pairwise import cosine_similarity

model_path = "F:\\conda_envs\\r1\\text2vec-base-chinese"
model = SentenceModel(model_path)

db = ProjectDatabase(
    host="localhost",
    user="root",
    password="123456",
    database="sts"
)

# ================== å…³é”®è¯æƒé‡é…ç½® ==================
CORE_TECH_KEYWORDS = {
    'æ™ºèƒ½', 'è‡ªåŠ¨', 'è¯†åˆ«', 'åˆ†æ', 'å­¦ä¹ ', 'æ¨è', 'ç”Ÿæˆ', 'é¢„æµ‹',
    'ç›‘æ§', 'ç›‘æµ‹', 'ç®¡ç†', 'å®‰å…¨', 'æ”¯ä»˜', 'äº¤æ˜“', 'åŠ å¯†', 'åŒæ­¥',
    'å¤‡ä»½', 'æ¢å¤', 'é›†æˆ', 'ç®—æ³•', 'æ¨¡å‹', 'æ·±åº¦', 'æ•°æ®', 'å®æ—¶'
}

BASIC_FUNCTION_KEYWORDS = {
    'æä¾›', 'æ”¯æŒ', 'å®ç°', 'è®°å½•', 'æŸ¥çœ‹', 'è®¾ç½®', 'é…ç½®', 'ä¿å­˜',
    'å¯¼å…¥', 'å¯¼å‡º', 'ç¼–è¾‘', 'è°ƒæ•´', 'åˆ›å»º', 'æ·»åŠ ', 'æœç´¢', 'ç­›é€‰',
    'åˆ†ç±»', 'æ•´ç†', 'è®¡ç®—', 'ç»Ÿè®¡', 'å±•ç¤º', 'ä¸Šä¼ ', 'ä¸‹è½½'
}

UX_KEYWORDS = {
    'ç•Œé¢', 'æ•™ç¨‹', 'å¸®åŠ©', 'åˆ†äº«', 'ç¤¾åŒº', 'ç¾åŒ–', 'é¢„è§ˆ', 'å±•ç¤º',
    'æé†’', 'é€šçŸ¥', 'å‹å¥½', 'ç®€æ´', 'æ–‡æ¡£', 'è¯´æ˜', 'å¼•å¯¼'
}

# ================== è½¯é˜ˆå€¼é…ç½® ==================
SOFT_THRESHOLD = 0.35  # é˜ˆå€¼ï¼šä½äºæ­¤å€¼çš„ç›¸ä¼¼åº¦ä¼šè¢«æƒ©ç½š
PENALTY_FACTOR = 0.5  # æƒ©ç½šå› å­ï¼šä½äºé˜ˆå€¼çš„ç›¸ä¼¼åº¦ä¹˜ä»¥è¿™ä¸ªç³»æ•°


def precise_keyword_weights(features):
    """åŸºäºçœŸå®æ•°æ®ç‰¹å¾çš„ç²¾å‡†æƒé‡åˆ†é…"""
    weights = []

    for feature in features:
        # ç»Ÿè®¡å„ç±»å…³é”®è¯
        core_terms = [word for word in CORE_TECH_KEYWORDS if word in feature]
        basic_terms = [word for word in BASIC_FUNCTION_KEYWORDS if word in feature]
        ux_terms = [word for word in UX_KEYWORDS if word in feature]

        # ç²¾å‡†æƒé‡é€»è¾‘
        if len(core_terms) >= 3:
            weight = 0.9  # å¤šä¸ªæ ¸å¿ƒæŠ€æœ¯è¯
        elif len(core_terms) == 2:
            weight = 0.8  # ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯è¯
        elif len(core_terms) == 1:
            if len(basic_terms) >= 1:
                weight = 0.7  # æŠ€æœ¯+åŠŸèƒ½ç»„åˆ
            else:
                weight = 0.6  # çº¯æŠ€æœ¯è¯
        elif len(basic_terms) >= 3:
            weight = 0.5  # å¤šä¸ªåŸºç¡€åŠŸèƒ½
        elif len(basic_terms) == 2:
            weight = 0.4  # ä¸¤ä¸ªåŸºç¡€åŠŸèƒ½
        elif len(basic_terms) == 1:
            weight = 0.35  # å•ä¸ªåŸºç¡€åŠŸèƒ½
        elif len(ux_terms) >= 1:
            weight = 0.3  # ç”¨æˆ·ä½“éªŒç›¸å…³
        else:
            weight = 0.4  # é»˜è®¤æƒé‡

        weights.append(weight)

    # å½’ä¸€åŒ–
    total = sum(weights)
    if total > 0:
        normalized_weights = [w / total for w in weights]
        print(f"ç‰¹å¾æƒé‡åˆ†é…: {normalized_weights}")
        return normalized_weights
    else:
        default_weights = [1 / len(features)] * len(features)
        print(f"ä½¿ç”¨é»˜è®¤æƒé‡: {default_weights}")
        return default_weights


def apply_soft_threshold(similarities, threshold=SOFT_THRESHOLD, penalty=PENALTY_FACTOR):
    """
    åº”ç”¨è½¯é˜ˆå€¼å¤„ç†
    - é«˜äºé˜ˆå€¼çš„ç›¸ä¼¼åº¦ï¼šå®Œå…¨ä¿ç•™
    - ä½äºé˜ˆå€¼çš„ç›¸ä¼¼åº¦ï¼šæŒ‰æƒ©ç½šå› å­è¡°å‡
    """
    adjusted_similarities = np.where(
        similarities >= threshold,
        similarities,  # é«˜äºé˜ˆå€¼ï¼šå®Œå…¨ä¿ç•™
        similarities * penalty  # ä½äºé˜ˆå€¼ï¼šæŒ‰æƒ©ç½šå› å­è¡°å‡
    )

    # æ‰“å°è°ƒæ•´è¯¦æƒ…
    original_mean = similarities.mean()
    adjusted_mean = adjusted_similarities.mean()
    print(f"è½¯é˜ˆå€¼å¤„ç†: {threshold} | æƒ©ç½šå› å­: {penalty}")
    print(f"ç›¸ä¼¼åº¦å‡å€¼: {original_mean:.4f} â†’ {adjusted_mean:.4f}")

    return adjusted_similarities


def print_matching_details(demand_feature, existing_features, similarities, adjusted_similarities=None):
    """æ‰“å°åŒ¹é…è¯¦æƒ…ï¼ŒåŒ…å«è½¯é˜ˆå€¼å¤„ç†å‰åçš„å¯¹æ¯”"""
    print(f"\néœ€æ±‚ç‰¹å¾: {demand_feature}")
    print("ä¸å·²æœ‰ç‰¹å¾çš„ç›¸ä¼¼åº¦:")
    for i, (existing_feature, similarity) in enumerate(zip(existing_features, similarities)):
        if adjusted_similarities is not None:
            adjusted = adjusted_similarities[i]
            marker = "âš ï¸" if similarity < SOFT_THRESHOLD else "âœ…"
            print(f"  {marker} {existing_feature}: {similarity:.4f} â†’ {adjusted:.4f}")
        else:
            print(f"  - {existing_feature}: {similarity:.4f}")


def match_projects(demand_project, existing_projects):
    """åŒ¹é…éœ€æ±‚é¡¹ç›®ä¸å·²æœ‰é¡¹ç›®ï¼ˆä½¿ç”¨è½¯é˜ˆå€¼ï¼‰"""
    print("\n=== å¼€å§‹é¡¹ç›®åŒ¹é…ï¼ˆè½¯é˜ˆå€¼æ–¹æ¡ˆï¼‰ ===")
    print(f"è½¯é˜ˆå€¼é…ç½®: é˜ˆå€¼={SOFT_THRESHOLD}, æƒ©ç½šå› å­={PENALTY_FACTOR}")
    print(f"éœ€æ±‚é¡¹ç›®ç‰¹å¾æ•°é‡: {len(demand_project['features'])}")

    # 1. å‘é‡åŒ–éœ€æ±‚é¡¹ç›®çš„æ‰€æœ‰åŠŸèƒ½
    print("\næ­£åœ¨å‘é‡åŒ–éœ€æ±‚é¡¹ç›®ç‰¹å¾...")
    demand_vectors = model.encode(demand_project["features"])

    # è®¡ç®—ç‰¹å¾æƒé‡
    weights = precise_keyword_weights(demand_project["features"])

    project_scores = {}
    for project in existing_projects:
        if not project["features"]:
            print(f"\né¡¹ç›® {project['id']} æ²¡æœ‰ç‰¹å¾ï¼Œè·³è¿‡")
            project_scores[project["id"]] = 0
            continue

        print(f"\næ­£åœ¨åŒ¹é…é¡¹ç›® {project['id']}")
        print(f"é¡¹ç›®ç‰¹å¾æ•°é‡: {len(project['features'])}")

        # 2. å‘é‡åŒ–å½“å‰å·²æœ‰é¡¹ç›®çš„æ‰€æœ‰åŠŸèƒ½
        existing_vectors = model.encode(project["features"])

        # 3. è®¡ç®—éœ€æ±‚åŠŸèƒ½ä¸å·²æœ‰åŠŸèƒ½çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(demand_vectors, existing_vectors)

        # 4. å¯¹æ¯ä¸ªéœ€æ±‚åŠŸèƒ½ï¼Œæ‰¾åˆ°å…¶åœ¨å·²æœ‰åŠŸèƒ½ä¸­çš„æœ€å¤§ç›¸ä¼¼åº¦
        max_sim_per_demand_feature = similarity_matrix.max(axis=1)

        # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šåº”ç”¨è½¯é˜ˆå€¼å¤„ç†
        adjusted_similarities = apply_soft_threshold(max_sim_per_demand_feature)

        # æ‰“å°åŒ¹é…è¯¦æƒ…ï¼ˆåŒ…å«è°ƒæ•´å‰åå¯¹æ¯”ï¼‰
        for i, (demand_feature, original_sim, adjusted_sim) in enumerate(
                zip(demand_project["features"], max_sim_per_demand_feature, adjusted_similarities)
        ):
            print(f"\néœ€æ±‚ç‰¹å¾ {i + 1}: {demand_feature}")
            print(f"åŸå§‹æœ€å¤§ç›¸ä¼¼åº¦: {original_sim:.4f}")
            print(f"è°ƒæ•´åç›¸ä¼¼åº¦: {adjusted_sim:.4f}")

            if original_sim < SOFT_THRESHOLD:
                print("âš ï¸  ä½äºé˜ˆå€¼ï¼Œå·²åº”ç”¨æƒ©ç½š")

            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„ç‰¹å¾
            best_match_idx = similarity_matrix[i].argmax()
            best_match_feature = project["features"][best_match_idx]
            print(f"æœ€åŒ¹é…çš„å·²æœ‰ç‰¹å¾: {best_match_feature}")

        # ä½¿ç”¨è°ƒæ•´åçš„ç›¸ä¼¼åº¦è®¡ç®—åŠ æƒå¹³å‡
        if sum(weights) > 0:
            final_score = np.average(adjusted_similarities, weights=weights)
        else:
            final_score = adjusted_similarities.mean()

        project_scores[project["id"]] = final_score
        print(f"\né¡¹ç›® {project['id']} çš„æœ€ç»ˆåŒ¹é…å¾—åˆ†: {final_score:.4f}")

    return project_scores


def evaluate_soft_threshold_effect(demand_project, existing_projects_sample):
    """è¯„ä¼°è½¯é˜ˆå€¼æ•ˆæœ"""
    print("\n" + "=" * 60)
    print("è½¯é˜ˆå€¼æ•ˆæœè¯„ä¼°")
    print("=" * 60)

    demand_vectors = model.encode(demand_project["features"])
    weights = precise_keyword_weights(demand_project["features"])

    for project in existing_projects_sample[:3]:  # è¯„ä¼°å‰3ä¸ªé¡¹ç›®
        print(f"\nè¯„ä¼°é¡¹ç›®: {project['id']}")

        existing_vectors = model.encode(project["features"])
        similarity_matrix = cosine_similarity(demand_vectors, existing_vectors)
        max_sim_per_demand_feature = similarity_matrix.max(axis=1)

        # è®¡ç®—ä¸‰ç§æ–¹æ¡ˆçš„å¾—åˆ†
        # 1. æ— é˜ˆå€¼
        no_threshold_score = np.average(max_sim_per_demand_feature, weights=weights)

        # 2. ç¡¬é˜ˆå€¼ï¼ˆåŸæ¥çš„æ–¹æ¡ˆï¼‰
        hard_threshold_sim = np.where(max_sim_per_demand_feature >= 0.5, max_sim_per_demand_feature, 0.0)
        hard_threshold_score = np.average(hard_threshold_sim, weights=weights)

        # 3. è½¯é˜ˆå€¼ï¼ˆæ–°æ–¹æ¡ˆï¼‰
        soft_threshold_sim = apply_soft_threshold(max_sim_per_demand_feature)
        soft_threshold_score = np.average(soft_threshold_sim, weights=weights)

        print(f"  æ— é˜ˆå€¼å¾—åˆ†: {no_threshold_score:.4f}")
        print(f"  ç¡¬é˜ˆå€¼å¾—åˆ†: {hard_threshold_score:.4f}")
        print(f"  è½¯é˜ˆå€¼å¾—åˆ†: {soft_threshold_score:.4f}")

        # ç»Ÿè®¡ä½äºé˜ˆå€¼çš„ç‰¹å¾æ•°é‡
        low_sim_count = (max_sim_per_demand_feature < SOFT_THRESHOLD).sum()
        total_count = len(max_sim_per_demand_feature)
        print(f"  ä½äºé˜ˆå€¼ç‰¹å¾: {low_sim_count}/{total_count} ({low_sim_count / total_count * 100:.1f}%)")


def main():
    print("æ­£åœ¨è·å–ç°æœ‰é¡¹ç›®...")
    existing_projects = []
    projects = db.get_all_projects()
    for project in projects:
        features = db.get_project_features(project['project_name'])
        if features:
            existing_projects.append({
                "id": project['project_id'],
                "name": project['project_name'],
                "features": features
            })

    print(f"æˆåŠŸè·å– {len(existing_projects)} ä¸ªç°æœ‰é¡¹ç›®")

    demand_document_path = "E:\\project\\Software_trading_system\\demand_document\\æˆ‘éœ€è¦APPå¼€å‘1.docx"
    print(f"\næ­£åœ¨å¤„ç†éœ€æ±‚æ–‡æ¡£: {demand_document_path}")

    features = extract_features_from_document(demand_document_path)
    if not features:
        raise Exception("éœ€æ±‚é¡¹ç›®ç‰¹å¾æå–å¤±è´¥")

    project_name = f"demand_{demand_document_path.split('/')[-1].split('.')[0]}"
    project_id = db.add_project(project_name, demand_document_path, features)

    if not project_id:
        raise Exception("éœ€æ±‚é¡¹ç›®å¤„ç†å¤±è´¥")

    demand_project = {
        "id": project_id,
        "name": project_name,
        "features": features
    }

    # å¯é€‰ï¼šè¯„ä¼°è½¯é˜ˆå€¼æ•ˆæœ
    if len(existing_projects) >= 3:
        evaluate_soft_threshold_effect(demand_project, existing_projects[:3])

    # æ‰§è¡ŒåŒ¹é…
    project_scores = match_projects(demand_project, existing_projects)

    sorted_projects = sorted(project_scores.items(), key=lambda item: item[1], reverse=True)

    print("\n=== æœ€ç»ˆåŒ¹é…ç»“æœï¼ˆè½¯é˜ˆå€¼æ–¹æ¡ˆï¼‰ ===")
    for rank, (project_id, score) in enumerate(sorted_projects[:10], 1):
        project_name = next((p["name"] for p in existing_projects if p["id"] == project_id), "æœªçŸ¥é¡¹ç›®")
        print(f"æ’å {rank}: {project_name} (ID: {project_id})")
        print(f"åŒ¹é…å¾—åˆ†: {score:.4f}")
        print("-" * 50)


def get_existing_projects():
    existing_projects = []
    projects = db.get_all_projects()
    for project in projects:
        features = db.get_project_features(project['project_name'])
        existing_projects.append({
            "id": project['project_id'],
            "name": project['project_name'],
            "features": features or []
        })
    return existing_projects


if __name__ == "__main__":
    main()